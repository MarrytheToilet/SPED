#!/usr/bin/env python3
"""
å®Œæ•´çš„æ•°æ®å¤„ç†Pipelineæµ‹è¯•å·¥å…·

åŠŸèƒ½ï¼š
1. æµ‹è¯•PDFä¿¡æ¯æå–ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰
2. æµ‹è¯•æ•°æ®åº“æ’å…¥ï¼ˆéœ€è¦SQLiteï¼‰
3. æ”¯æŒè¯¦ç»†/ç®€æ´ä¸¤ç§è¾“å‡ºæ¨¡å¼
4. æä¾›äº¤äº’å¼é€‰æ‹©

ç”¨æ³•ï¼š
    python scripts/test_pipeline.py              # äº¤äº’å¼æ¨¡å¼
    python scripts/test_pipeline.py --extract    # åªæµ‹è¯•æå–
    python scripts/test_pipeline.py --database   # å®Œæ•´æµ‹è¯•ï¼ˆå«æ•°æ®åº“ï¼‰
    python scripts/test_pipeline.py --simple     # ç®€æ´è¾“å‡ºæ¨¡å¼
"""
import sys
import json
import argparse
from pathlib import Path
from loguru import logger

sys.path.insert(0, str(Path(__file__).parent))

from src.agents.llm_agent import LLMExtractionAgent
from src.extractors.text_extractor import TextExtractor
from scripts.process_pipeline import ProcessingPipeline
from settings import PARSED_DIR, SCHEMA_DIR


def setup_logger(verbose=True):
    """é…ç½®æ—¥å¿—è¾“å‡º"""
    logger.remove()
    
    if verbose:
        # è¯¦ç»†æ¨¡å¼
        logger.add(
            sys.stdout,
            level="INFO",
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>"
        )
    else:
        # ç®€æ´æ¨¡å¼
        logger.add(
            sys.stdout,
            level="INFO",
            format="<level>{message}</level>"
        )
    
    # æ–‡ä»¶æ—¥å¿—ï¼ˆæ€»æ˜¯è¯¦ç»†ï¼‰
    logger.add(
        "logs/test_pipeline_{time}.log",
        rotation="10 MB",
        level="DEBUG",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
    )


def select_test_paper():
    """é€‰æ‹©ä¸€ç¯‡æµ‹è¯•è®ºæ–‡"""
    batch_dir = PARSED_DIR / "output" / "batch_1"
    
    if not batch_dir.exists():
        batch_dir = PARSED_DIR / "batch_1"  # å¤‡ç”¨è·¯å¾„
    
    if not batch_dir.exists():
        logger.error(f"æ‰¹æ¬¡ç›®å½•ä¸å­˜åœ¨: {batch_dir}")
        return None
    
    paper_dirs = [d for d in batch_dir.iterdir() if d.is_dir()]
    
    if not paper_dirs:
        logger.error("æœªæ‰¾åˆ°è§£æçš„è®ºæ–‡")
        return None
    
    return paper_dirs[0]


def test_extraction_only(paper_dir: Path, verbose: bool = True):
    """æµ‹è¯•æå–åŠŸèƒ½ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰"""
    
    if verbose:
        logger.info("=" * 80)
        logger.info("æµ‹è¯•æ¨¡å¼: ä»…æå–ï¼ˆæ— æ•°æ®åº“ï¼‰")
        logger.info("=" * 80)
    
    logger.info(f"\nğŸ“„ æµ‹è¯•è®ºæ–‡: {paper_dir.name}")
    
    # 1. æå–æ–‡æœ¬
    if verbose:
        logger.info("\nğŸ” æ­¥éª¤1: ä»è§£æç»“æœä¸­æå–æ–‡æœ¬...")
    
    extractor = TextExtractor()
    data = extractor.extract_from_parsed(str(paper_dir))
    
    title = data.get('metadata', {}).get('title', 'æœªæå–åˆ°æ ‡é¢˜')
    sections = data.get('sections', {})
    
    if verbose:
        logger.info(f"   âœ“ è®ºæ–‡æ ‡é¢˜: {title}")
        logger.info(f"   âœ“ ç« èŠ‚æ•°: {len(sections)}")
        logger.info(f"   âœ“ æ€»å­—æ•°: {sum(len(content) for content in sections.values())}")
        
        if sections:
            logger.info(f"\n   ç« èŠ‚åˆ—è¡¨:")
            for i, (sec_name, content) in enumerate(list(sections.items())[:10], 1):
                logger.info(f"      {i}. {sec_name} ({len(content)} å­—ç¬¦)")
            if len(sections) > 10:
                logger.info(f"      ... è¿˜æœ‰ {len(sections) - 10} ä¸ªç« èŠ‚")
    else:
        logger.info(f"ç« èŠ‚æ•°: {len(sections)}, æ€»å­—æ•°: {sum(len(content) for content in sections.values())}")
    
    # 2. ä½¿ç”¨LLM Agentæå–ç»“æ„åŒ–ä¿¡æ¯
    if verbose:
        logger.info("\nğŸ¤– æ­¥éª¤2: ä½¿ç”¨ AI æå–ç»“æ„åŒ–ä¿¡æ¯...")
        logger.info("   æç¤º: è¿™ä¸€æ­¥ä¼šè°ƒç”¨ OpenAI APIï¼Œå¯èƒ½éœ€è¦1-2åˆ†é’Ÿ")
    else:
        logger.info("\nå¼€å§‹AIæå–...")
    
    schema_file = SCHEMA_DIR / "inferred_schema.json"
    if not schema_file.exists():
        logger.error(f"Schema æ–‡ä»¶ä¸å­˜åœ¨: {schema_file}")
        return None
    
    agent = LLMExtractionAgent(schema_path=str(schema_file))
    
    input_data = {
        "sections": sections,
        "paper_id": paper_dir.name
    }
    
    try:
        result = agent.process(input_data)
        
        # 3. ä¿å­˜ç»“æœ
        output_file = Path("logs") / f"extraction_{paper_dir.name}.json"
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        if verbose:
            logger.info(f"\nâœ… æå–æˆåŠŸï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            logger.info(f"ç»“æœå·²ä¿å­˜: {output_file}")
        
        # 4. åˆ†ææå–ç»“æœ
        analyze_extraction_result(result, verbose)
        
        return result
        
    except Exception as e:
        logger.error(f"\nâœ— æå–å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def analyze_extraction_result(result: dict, verbose: bool = True):
    """åˆ†æå¹¶æ˜¾ç¤ºæå–ç»“æœ"""
    
    if verbose:
        logger.info("\nğŸ“Š æå–ç»“æœåˆ†æ:")
        logger.info("=" * 80)
    else:
        logger.info("\næå–ç»“æœ:")
    
    if result.get("extraction_type") == "multi_experiment":
        # å¤šç»„å®éªŒ
        total = result.get("total_experiments", 0)
        experiments = result.get("experiments", [])
        
        logger.info(f"   æ£€æµ‹åˆ°å¤šç»„å®éªŒ: {total} ç»„")
        
        if verbose:
            logger.info("")
            for idx, exp in enumerate(experiments, 1):
                logger.info(f"   å®éªŒç»„ {idx}:")
                logger.info(f"      ID: {exp.get('exp_id')}")
                logger.info(f"      DataID: {exp.get('dataid')}")
                
                non_null_fields = {k: v for k, v in exp.items() 
                                  if v is not None and v != "" 
                                  and k not in ["exp_id", "dataid"]}
                
                logger.info(f"      æå–å­—æ®µæ•°: {len(non_null_fields)}")
                
                # æ˜¾ç¤ºå…³é”®å­—æ®µ
                key_fields = ["æ•°æ®æ ‡è¯†", "åº”ç”¨éƒ¨ä½", "çƒå¤´åŸºæœ¬ä¿¡æ¯", "å†…è¡¬åŸºæœ¬ä¿¡æ¯"]
                for field in key_fields:
                    if field in exp and exp[field]:
                        value = str(exp[field])[:50] + "..." if len(str(exp[field])) > 50 else exp[field]
                        logger.info(f"         âœ“ {field}: {value}")
                
                logger.info("")
        else:
            for idx, exp in enumerate(experiments, 1):
                non_null = sum(1 for v in exp.values() if v is not None and v != "")
                logger.info(f"  å®éªŒ {idx}: {exp.get('exp_id')} | dataid={exp.get('dataid')} | {non_null}ä¸ªå­—æ®µ")
    
    else:
        # å•ç»„å®éªŒ
        logger.info(f"   æ£€æµ‹åˆ°å•ç»„å®éªŒ")
        logger.info(f"   DataID: {result.get('dataid')}")
        
        non_null_fields = {k: v for k, v in result.items() 
                          if v is not None and v != "" 
                          and k not in ["extraction_type", "paper_id", "dataid"]}
        
        logger.info(f"   æå–å­—æ®µæ•°: {len(non_null_fields)}")
        
        if verbose:
            # å­—æ®µåˆ†ç±»ç»Ÿè®¡
            categories = {}
            for field in non_null_fields.keys():
                parts = field.split('_')
                if len(parts) > 0:
                    category = parts[0]
                    categories[category] = categories.get(category, 0) + 1
            
            logger.info(f"\n   å­—æ®µåˆ†ç±»ç»Ÿè®¡:")
            for category, count in sorted(categories.items()):
                logger.info(f"      {category}: {count} ä¸ªå­—æ®µ")
    
    if verbose:
        logger.info("\n" + "=" * 80)


def test_database_insertion(paper_dir: Path, verbose: bool = True):
    """æµ‹è¯•æ•°æ®åº“æ’å…¥åŠŸèƒ½"""
    
    if verbose:
        logger.info("\n" + "=" * 80)
        logger.info("æµ‹è¯•æ¨¡å¼: å®Œæ•´æµç¨‹ï¼ˆåŒ…å«æ•°æ®åº“ï¼‰")
        logger.info("=" * 80)
        logger.warning("âš ï¸  æ³¨æ„ï¼šæ­¤æ“ä½œä¼šçœŸå®å†™å…¥æ•°æ®åº“ï¼")
    else:
        logger.info("\næµ‹è¯•æ•°æ®åº“æ’å…¥...")
    
    try:
        pipeline = ProcessingPipeline(use_database=True)
        result = pipeline.process_paper(str(paper_dir))
        
        if "error" in result:
            logger.error(f"å¤„ç†å¤±è´¥: {result['error']}")
            return False
        
        if verbose:
            logger.info("\n" + "=" * 80)
            logger.info("å¤„ç†ç»“æœ")
            logger.info("=" * 80)
        
        # æ˜¾ç¤ºç»“æœ
        logger.info(f"Paper ID: {result.get('paper_id', 'N/A')}")
        logger.info(f"æå–ç±»å‹: {result.get('extraction_type', 'N/A')}")
        
        if "database_insertion" in result:
            db_info = result["database_insertion"]
            success = db_info.get('success', False)
            
            if success:
                logger.info(f"âœ… æ•°æ®åº“æ’å…¥æˆåŠŸ")
            else:
                logger.error(f"âŒ æ•°æ®åº“æ’å…¥å¤±è´¥")
            
            if db_info.get('extraction_type') == 'multi_experiment':
                success_count = db_info.get('success_count', 0)
                total = db_info.get('total_experiments', 0)
                logger.info(f"æˆåŠŸæ’å…¥: {success_count}/{total} ç»„å®éªŒ")
                
                if verbose:
                    for res in db_info.get('results', []):
                        status = "âœ“" if res.get('success') else "âœ—"
                        logger.info(f"  {status} {res.get('exp_id')}: dataid={res.get('dataid')}")
            else:
                logger.info(f"Data ID: {db_info.get('dataid', 'N/A')}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = Path("logs") / f"database_test_{paper_dir.name}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        if verbose:
            logger.info(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        return True
        
    except Exception as e:
        logger.exception(f"æ•°æ®åº“æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    setup_logger(verbose=True)
    
    logger.info("=" * 80)
    logger.info("PDF æ•°æ®å¤„ç† Pipeline æµ‹è¯•å·¥å…·")
    logger.info("=" * 80)
    
    # é€‰æ‹©æµ‹è¯•è®ºæ–‡
    paper_dir = select_test_paper()
    if not paper_dir:
        return
    
    logger.info("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    logger.info("  1. ä»…æµ‹è¯•æå–ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰")
    logger.info("  2. æµ‹è¯•æå– + æ•°æ®åº“æ’å…¥ï¼ˆéœ€è¦SQLiteï¼‰")
    logger.info("  3. é€€å‡º")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-3): ").strip()
    
    if choice == "1":
        test_extraction_only(paper_dir, verbose=True)
        logger.info("\nâœ… æå–æµ‹è¯•å®Œæˆï¼")
        logger.info("\nğŸ’¡ æç¤º:")
        logger.info("   - å¦‚éœ€æµ‹è¯•æ•°æ®åº“æ’å…¥ï¼Œè¯·é‡æ–°è¿è¡Œå¹¶é€‰æ‹©é€‰é¡¹2")
        logger.info("   - æˆ–ç›´æ¥è¿è¡Œ: python test_pipeline.py --database")
    
    elif choice == "2":
        logger.info("\nå¼€å§‹å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆæå– + æ•°æ®åº“æ’å…¥ï¼‰...")
        
        # ç›´æ¥è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆtest_database_insertionå†…éƒ¨ä¼šè°ƒç”¨pipelineï¼ŒåŒ…å«æå–å’Œæ’å…¥ï¼‰
        success = test_database_insertion(paper_dir, verbose=True)
        
        if success:
            logger.info("\nâœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸï¼")
        else:
            logger.error("\nâŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥")
    
    elif choice == "3":
        logger.info("é€€å‡º")
    
    else:
        logger.error("æ— æ•ˆçš„é€‰é¡¹")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFæ•°æ®å¤„ç†Pipelineæµ‹è¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python scripts/test_pipeline.py              # äº¤äº’å¼æ¨¡å¼
  python scripts/test_pipeline.py --extract    # åªæµ‹è¯•æå–
  python scripts/test_pipeline.py --database   # å®Œæ•´æµ‹è¯•ï¼ˆå«æ•°æ®åº“ï¼‰
  python scripts/test_pipeline.py --simple     # ç®€æ´è¾“å‡º
        """
    )
    
    parser.add_argument(
        '--extract',
        action='store_true',
        help='åªæµ‹è¯•æå–åŠŸèƒ½ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰'
    )
    
    parser.add_argument(
        '--database',
        action='store_true',
        help='æµ‹è¯•å®Œæ•´æµç¨‹ï¼ˆåŒ…å«æ•°æ®åº“æ’å…¥ï¼‰'
    )
    
    parser.add_argument(
        '--simple',
        action='store_true',
        help='ä½¿ç”¨ç®€æ´è¾“å‡ºæ¨¡å¼'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‚æ•°ï¼Œä½¿ç”¨äº¤äº’å¼æ¨¡å¼
    if not (args.extract or args.database):
        interactive_mode()
        return
    
    # è®¾ç½®æ—¥å¿—
    verbose = not args.simple
    setup_logger(verbose=verbose)
    
    # é€‰æ‹©æµ‹è¯•è®ºæ–‡
    paper_dir = select_test_paper()
    if not paper_dir:
        sys.exit(1)
    
    # æ‰§è¡Œæµ‹è¯•
    if args.extract:
        result = test_extraction_only(paper_dir, verbose=verbose)
        if result:
            logger.info("\nâœ… æå–æµ‹è¯•å®Œæˆï¼")
            sys.exit(0)
        else:
            logger.error("\nâŒ æå–æµ‹è¯•å¤±è´¥")
            sys.exit(1)
    
    elif args.database:
        # å…ˆæå–
        logger.info("æ­¥éª¤1: æµ‹è¯•æå–...")
        extraction_result = test_extraction_only(paper_dir, verbose=verbose)
        
        if not extraction_result:
            logger.error("æå–å¤±è´¥")
            sys.exit(1)
        
        # å†æµ‹è¯•æ•°æ®åº“
        logger.info("\næ­¥éª¤2: æµ‹è¯•æ•°æ®åº“æ’å…¥...")
        success = test_database_insertion(paper_dir, verbose=verbose)
        
        if success:
            logger.info("\nâœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸï¼")
            sys.exit(0)
        else:
            logger.error("\nâŒ æ•°æ®åº“æµ‹è¯•å¤±è´¥")
            sys.exit(1)


if __name__ == "__main__":
    main()
