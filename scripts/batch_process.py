#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†è„šæœ¬ - å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFè§£æç»“æœ

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨æ‰«ææŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰è®ºæ–‡
2. é€ä¸€æå–ä¿¡æ¯å¹¶æ’å…¥æ•°æ®åº“
3. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å¤„ç†çš„è®ºæ–‡ï¼‰

ç”¨æ³•ï¼š
    python scripts/batch_process.py --folder batch_1           # å¤„ç†æŒ‡å®šbatch
    python scripts/batch_process.py --folder batch_1 --skip-db  # åªæå–ï¼Œä¸å†™æ•°æ®åº“
    python scripts/batch_process.py --folder batch_1 --resume   # ç»­ä¼ æ¨¡å¼ï¼ˆè·³è¿‡å·²å¤„ç†ï¼‰
"""

import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
from loguru import logger
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
sys.path.insert(0, str(Path(__file__).parent.parent))

from settings import PARSED_DIR, ANALYZED_DIR
from scripts.process_pipeline import ProcessingPipeline


class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, use_database: bool = True, resume: bool = False):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            use_database: æ˜¯å¦å†™å…¥æ•°æ®åº“
            resume: æ˜¯å¦ç»­ä¼ æ¨¡å¼ï¼ˆè·³è¿‡å·²å¤„ç†çš„è®ºæ–‡ï¼‰
        """
        self.pipeline = ProcessingPipeline(use_database=use_database)
        self.use_database = use_database
        self.resume = resume
        self.results = []
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0
        }
    
    def is_already_processed(self, paper_id: str) -> bool:
        """æ£€æŸ¥è®ºæ–‡æ˜¯å¦å·²ç»å¤„ç†è¿‡"""
        output_dir = ANALYZED_DIR / paper_id
        result_file = output_dir / "extraction_results.json"
        return result_file.exists()
    
    def process_folder(self, folder_name: str) -> Dict[str, Any]:
        """
        å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è®ºæ–‡
        
        Args:
            folder_name: æ–‡ä»¶å¤¹åç§°ï¼ˆä¾‹å¦‚ï¼šbatch_1ï¼‰
        
        Returns:
            å¤„ç†æŠ¥å‘Š
        """
        folder_path = PARSED_DIR / folder_name
        
        if not folder_path.exists():
            logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            return {"error": "æ–‡ä»¶å¤¹ä¸å­˜åœ¨"}
        
        # è·å–æ‰€æœ‰è®ºæ–‡ç›®å½•
        paper_dirs = sorted([d for d in folder_path.iterdir() if d.is_dir()])
        
        if not paper_dirs:
            logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ç›®å½•: {folder_path}")
            return {"error": "æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡"}
        
        self.stats['total'] = len(paper_dirs)
        
        logger.info("=" * 80)
        logger.info(f"ğŸ“‚ æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹: {folder_name}")
        logger.info(f"ğŸ“„ æ‰¾åˆ°è®ºæ–‡æ•°é‡: {len(paper_dirs)}")
        logger.info(f"ğŸ’¾ æ•°æ®åº“æ¨¡å¼: {'å¼€å¯' if self.use_database else 'å…³é—­'}")
        logger.info(f"ğŸ”„ ç»­ä¼ æ¨¡å¼: {'å¼€å¯' if self.resume else 'å…³é—­'}")
        logger.info("=" * 80)
        
        # å¼€å§‹å¤„ç†
        start_time = datetime.now()
        
        for i, paper_dir in enumerate(paper_dirs, 1):
            paper_id = paper_dir.name
            
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ“„ [{i}/{len(paper_dirs)}] å¤„ç†è®ºæ–‡: {paper_id}")
            logger.info(f"{'='*80}")
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆç»­ä¼ æ¨¡å¼ï¼‰
            if self.resume and self.is_already_processed(paper_id):
                logger.info(f"â­ï¸  è®ºæ–‡å·²å¤„ç†ï¼Œè·³è¿‡: {paper_id}")
                self.stats['skipped'] += 1
                self.results.append({
                    'paper_id': paper_id,
                    'status': 'skipped',
                    'message': 'å·²å¤„ç†'
                })
                continue
            
            # å¤„ç†è®ºæ–‡
            try:
                result = self.pipeline.process_paper(str(paper_dir), paper_id)
                
                if "error" in result:
                    logger.error(f"âŒ å¤„ç†å¤±è´¥: {result['error']}")
                    self.stats['failed'] += 1
                    self.results.append({
                        'paper_id': paper_id,
                        'status': 'failed',
                        'error': result['error']
                    })
                else:
                    logger.success(f"âœ… å¤„ç†æˆåŠŸ: {paper_id}")
                    self.stats['success'] += 1
                    
                    # ç»Ÿè®¡æå–çš„æ•°æ®
                    extraction_info = self._get_extraction_info(result)
                    
                    self.results.append({
                        'paper_id': paper_id,
                        'status': 'success',
                        'extraction_info': extraction_info
                    })
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
                logger.exception(e)
                self.stats['failed'] += 1
                self.results.append({
                    'paper_id': paper_id,
                    'status': 'failed',
                    'error': str(e)
                })
        
        # å¤„ç†å®Œæˆ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(folder_name, duration)
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report, folder_name)
        
        return report
    
    def _get_extraction_info(self, result: Dict) -> Dict:
        """è·å–æå–ä¿¡æ¯æ‘˜è¦"""
        info = {
            'extraction_type': result.get('extraction_type', 'unknown'),
            'data_count': 0,
            'field_count': 0
        }
        
        # ç»Ÿè®¡æå–çš„æ•°æ®
        if result.get('extraction_type') == 'multi_experiment':
            experiments = result.get('llm_extraction', {}).get('experiments', [])
            info['data_count'] = len(experiments)
            if experiments:
                # ç»Ÿè®¡ç¬¬ä¸€ä¸ªå®éªŒçš„å­—æ®µæ•°
                first_exp = experiments[0]
                info['field_count'] = len([v for v in first_exp.values() if v])
        else:
            llm_result = result.get('llm_extraction', {})
            info['data_count'] = 1
            info['field_count'] = len([v for v in llm_result.values() if v])
        
        # æ•°æ®åº“æ’å…¥ä¿¡æ¯
        if 'database_insertion' in result:
            db_info = result['database_insertion']
            info['db_success'] = db_info.get('success', False)
            if db_info.get('extraction_type') == 'multi_experiment':
                info['db_inserted'] = db_info.get('success_count', 0)
        
        return info
    
    def _generate_report(self, folder_name: str, duration: float) -> Dict:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report = {
            'folder': folder_name,
            'timestamp': datetime.now().isoformat(),
            'duration_seconds': round(duration, 2),
            'statistics': self.stats.copy(),
            'success_rate': round(self.stats['success'] / self.stats['total'] * 100, 2) if self.stats['total'] > 0 else 0,
            'results': self.results
        }
        
        # æ‰“å°æ‘˜è¦
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š å¤„ç†æŠ¥å‘Šæ‘˜è¦")
        logger.info("=" * 80)
        logger.info(f"æ–‡ä»¶å¤¹: {folder_name}")
        logger.info(f"æ€»æ•°: {self.stats['total']}")
        logger.info(f"æˆåŠŸ: {self.stats['success']} âœ…")
        logger.info(f"å¤±è´¥: {self.stats['failed']} âŒ")
        logger.info(f"è·³è¿‡: {self.stats['skipped']} â­ï¸")
        logger.info(f"æˆåŠŸç‡: {report['success_rate']}%")
        logger.info(f"è€—æ—¶: {duration:.2f} ç§’")
        logger.info(f"å¹³å‡é€Ÿåº¦: {duration/self.stats['total']:.2f} ç§’/ç¯‡" if self.stats['total'] > 0 else "N/A")
        logger.info("=" * 80)
        
        return report
    
    def _save_report(self, report: Dict, folder_name: str):
        """ä¿å­˜å¤„ç†æŠ¥å‘Š"""
        report_dir = Path("logs/batch_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"batch_report_{folder_name}_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡å¤„ç†PDFè§£æç»“æœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  python scripts/batch_process.py --folder batch_1
  python scripts/batch_process.py --folder batch_1 --skip-db
  python scripts/batch_process.py --folder batch_1 --resume
        """
    )
    
    parser.add_argument(
        "--folder",
        type=str,
        required=True,
        help="è¦å¤„ç†çš„æ–‡ä»¶å¤¹åç§°ï¼ˆä¾‹å¦‚ï¼šbatch_1ï¼‰"
    )
    
    parser.add_argument(
        "--skip-db",
        action="store_true",
        help="è·³è¿‡æ•°æ®åº“æ’å…¥ï¼ˆåªæå–ä¿¡æ¯ï¼‰"
    )
    
    parser.add_argument(
        "--resume",
        action="store_true",
        help="ç»­ä¼ æ¨¡å¼ï¼ˆè·³è¿‡å·²å¤„ç†çš„è®ºæ–‡ï¼‰"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logger.remove()
    
    if args.verbose:
        log_format = "<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>"
        logger.add(sys.stdout, level="DEBUG", format=log_format)
    else:
        log_format = "<level>{level: <8}</level> | <level>{message}</level>"
        logger.add(sys.stdout, level="INFO", format=log_format)
    
    # æ–‡ä»¶æ—¥å¿—
    log_file = f"logs/batch_process_{args.folder}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger.add(
        log_file,
        rotation="100 MB",
        retention="30 days",
        level="DEBUG",
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>"
    )
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    processor = BatchProcessor(
        use_database=not args.skip_db,
        resume=args.resume
    )
    
    # å¤„ç†æ–‡ä»¶å¤¹
    try:
        report = processor.process_folder(args.folder)
        
        if "error" not in report:
            logger.success("âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
            sys.exit(0)
        else:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {report['error']}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
        logger.exception(e)
        sys.exit(1)


if __name__ == "__main__":
    main()
