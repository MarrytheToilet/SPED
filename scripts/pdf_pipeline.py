#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„PDFå¤„ç†Pipeline - æ™ºèƒ½å»é‡ç‰ˆæœ¬
åŠŸèƒ½ï¼š
1. ä¸Šä¼ PDFï¼ˆè‡ªåŠ¨å»é‡ï¼Œå·²ä¸Šä¼ çš„ç§»åˆ°processedç›®å½•ï¼‰
2. æŸ¥è¯¢çŠ¶æ€
3. ä¸‹è½½ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼Œè·³è¿‡å·²ä¸‹è½½ï¼‰
"""
import os, sys, csv, json, time, shutil, requests, zipfile, argparse
from math import ceil
from pathlib import Path
from datetime import datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

try:
    from dotenv import load_dotenv
    load_dotenv(Path(__file__).parent.parent / '.env')
except: pass

sys.path.insert(0, str(Path(__file__).parent.parent))

# ==================== é…ç½® ====================
MINERU_API_BASE = os.getenv("MINERU_API_BASE", "https://mineru.net/api/v4")
MINERU_TOKEN = os.getenv("MINERU_TOKEN", "")
HEADERS = {"Authorization": f"Bearer {MINERU_TOKEN}", "Content-Type": "application/json"}

PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
PDF_DIR = DATA_DIR / "raw" / "pdfs"
PROCESSED_PDF_DIR = DATA_DIR / "raw" / "pdfs_processed"  # å·²å¤„ç†çš„PDF
OUTPUT_DIR = DATA_DIR / "processed" / "parsed" / "output"
BATCH_CSV = DATA_DIR / "uploads" / "upload_batches.csv"
STATUS_JSON = DATA_DIR / "uploads" / "processing_status.json"  # çŠ¶æ€è¿½è¸ª

for d in [PDF_DIR, PROCESSED_PDF_DIR, OUTPUT_DIR, BATCH_CSV.parent]:
    Path(d).mkdir(parents=True, exist_ok=True)

BATCH_SIZE = 10
UPLOAD_CONFIG = {"parse_method": "auto", "apply_ocr": False}
FILE_CONFIG = {"parse_method": "auto", "apply_ocr": False}

# ==================== çŠ¶æ€ç®¡ç†å™¨ ====================
class StatusManager:
    """å¤„ç†çŠ¶æ€ç®¡ç†å™¨ - é¿å…é‡å¤å¤„ç†"""
    
    def __init__(self):
        self.status_file = STATUS_JSON
        self.status = self._load()
    
    def _load(self):
        if self.status_file.exists():
            with open(self.status_file) as f:
                return json.load(f)
        return {"uploaded": {}, "downloaded": [], "analyzed": []}
    
    def _save(self):
        with open(self.status_file, 'w') as f:
            json.dump(self.status, f, ensure_ascii=False, indent=2)
    
    def is_uploaded(self, pdf_name):
        """æ£€æŸ¥PDFæ˜¯å¦å·²ä¸Šä¼ """
        return pdf_name in self.status["uploaded"]
    
    def mark_uploaded(self, pdf_name, batch_id):
        """æ ‡è®°PDFå·²ä¸Šä¼ """
        self.status["uploaded"][pdf_name] = batch_id
        self._save()
    
    def is_downloaded(self, batch_id):
        """æ£€æŸ¥batchæ˜¯å¦å·²ä¸‹è½½"""
        return batch_id in self.status["downloaded"]
    
    def mark_downloaded(self, batch_id):
        """æ ‡è®°batchå·²ä¸‹è½½"""
        if batch_id not in self.status["downloaded"]:
            self.status["downloaded"].append(batch_id)
            self._save()
    
    def get_stats(self):
        return {
            "uploaded": len(self.status["uploaded"]),
            "downloaded": len(self.status["downloaded"]),
            "analyzed": len(self.status["analyzed"])
        }

status_mgr = StatusManager()

# ==================== å·¥å…·å‡½æ•° ====================
def create_session():
    s = requests.Session()
    s.mount('http://', HTTPAdapter(max_retries=Retry(total=3)))
    s.mount('https://', HTTPAdapter(max_retries=Retry(total=3)))
    return s

def sanitize(name):
    return "".join(c if c.isalnum() or c in "._-" else "_" for c in Path(name).stem)

def download_file(url, path):
    for i in range(3):
        try:
            r = requests.get(url, timeout=120, stream=True)
            if r.status_code == 200:
                with open(path, 'wb') as f:
                    for chunk in r.iter_content(8192):
                        f.write(chunk)
                return True
        except: time.sleep(2)
    return False

def unzip_file(zip_path, dest):
    try:
        with zipfile.ZipFile(zip_path) as z:
            z.extractall(dest)
        return True
    except: return False

# ==================== ä¸Šä¼ å‘½ä»¤ ====================
def cmd_upload(args):
    pdf_dir = Path(args.input) if args.input else PDF_DIR
    pdfs = list(pdf_dir.glob("*.pdf"))
    
    if not pdfs:
        print(f"âš ï¸  æœªæ‰¾åˆ°PDF: {pdf_dir}")
        return
    
    stats = status_mgr.get_stats()
    print(f"\nğŸ“Š çŠ¶æ€ç»Ÿè®¡:")
    print(f"   å·²ä¸Šä¼ : {stats['uploaded']} ä¸ªPDF")
    print(f"   å·²ä¸‹è½½: {stats['downloaded']} ä¸ªæ‰¹æ¬¡")
    print(f"   å·²åˆ†æ: {stats['analyzed']} ç¯‡è®ºæ–‡")
    print(f"\nğŸ“ å‘ç° {len(pdfs)} ä¸ªPDFï¼Œåˆ† {ceil(len(pdfs)/BATCH_SIZE)} æ‰¹ä¸Šä¼ \n")
    
    # æ£€æŸ¥å·²æœ‰çš„batch IDï¼Œé¿å…é‡å¤
    existing_batch_ids = set()
    if BATCH_CSV.exists():
        with open(BATCH_CSV) as f:
            reader = csv.DictReader(f)
            for row in reader:
                if 'batch_id' in row:
                    existing_batch_ids.add(row['batch_id'])
        print(f"ğŸ” å·²æœ‰æ‰¹æ¬¡è®°å½•: {len(existing_batch_ids)} ä¸ª")
        if existing_batch_ids:
            print(f"   æœ€è¿‘æ‰¹æ¬¡: {list(existing_batch_ids)[-3:]}")
        print()
    else:
        with open(BATCH_CSV, 'w') as f:
            csv.writer(f).writerow(["batch_index", "batch_id", "file_count", "access_url", "time"])
    
    session = create_session()
    uploaded_total = 0
    skipped_total = 0
    
    for batch_idx in range(ceil(len(pdfs)/BATCH_SIZE)):
        batch = pdfs[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]
        
        # è¿‡æ»¤å·²ä¸Šä¼ çš„PDF
        to_upload = []
        for pdf in batch:
            if status_mgr.is_uploaded(pdf.name):
                print(f"â­ï¸  è·³è¿‡ï¼ˆå·²ä¸Šä¼ ï¼‰: {pdf.name}")
                skipped_total += 1
            else:
                to_upload.append(pdf)
        
        if not to_upload:
            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx+1}: å…¨éƒ¨å·²ä¸Šä¼ ï¼Œè·³è¿‡\n")
            continue
        
        print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_idx+1}: {len(to_upload)} ä¸ªæ–°æ–‡ä»¶ï¼ˆè·³è¿‡{len(batch)-len(to_upload)}ä¸ªï¼‰")
        
        # å‡†å¤‡ä¸Šä¼ æ•°æ®
        files_data = []
        for i, p in enumerate(to_upload):
            name = p.stem[:60] if len(p.stem) > 60 else p.stem
            files_data.append({
                "name": p.name,
                "data_id": f"b{batch_idx+1}_{i+1}_{name}",
                **FILE_CONFIG
            })
        
        try:
            # ç”³è¯·ä¸Šä¼ 
            r = session.post(f"{MINERU_API_BASE}/file-urls/batch",
                           headers=HEADERS,
                           json={**UPLOAD_CONFIG, "files": files_data},
                           timeout=30)
            
            if r.status_code != 200 or r.json().get("code") != 0:
                print(f"  âŒ ç”³è¯·å¤±è´¥: {r.json().get('msg', 'Unknown error')}")
                continue
            
            batch_id = r.json()["data"]["batch_id"]
            urls = r.json()["data"]["file_urls"]
            
            # æ£€æŸ¥batch_idæ˜¯å¦é‡å¤
            if batch_id in existing_batch_ids:
                print(f"  âš ï¸  è­¦å‘Šï¼šbatch_id é‡å¤ï¼{batch_id}")
                print(f"  æ­¤batchå·²å­˜åœ¨äºupload_batches.csvä¸­")
                print(f"  è·³è¿‡ä¿å­˜åˆ°CSVï¼Œä½†ç»§ç»­ä¸Šä¼ ...")
            
            # ä¸Šä¼ æ–‡ä»¶å¹¶ç§»åŠ¨
            success = 0
            for pdf, url in zip(to_upload, urls):
                with open(pdf, 'rb') as f:
                    if session.put(url, data=f, timeout=120).status_code in [200, 201]:
                        # æ ‡è®°å·²ä¸Šä¼ 
                        status_mgr.mark_uploaded(pdf.name, batch_id)
                        # ç§»åŠ¨åˆ°processedç›®å½•
                        dest = PROCESSED_PDF_DIR / pdf.name
                        shutil.move(str(pdf), str(dest))
                        success += 1
                        uploaded_total += 1
                        print(f"  âœ… {pdf.name} â†’ å·²ç§»è‡³ pdfs_processed/")
                    else:
                        print(f"  âŒ {pdf.name}")
            
            print(f"  å®Œæˆ: {success}/{len(to_upload)}")
            print(f"  Batch ID: {batch_id}")
            print(f"  è®¿é—®: https://mineru.net/extract/batch/{batch_id}")
            print(f"  â„¹ï¸  MinerUå°†è‡ªåŠ¨å¼€å§‹å¤„ç†æ‰¹æ¬¡")
            
            # ä¿å­˜æ‰¹æ¬¡è®°å½•ï¼ˆåªæœ‰éé‡å¤çš„æ‰ä¿å­˜ï¼‰
            if batch_id not in existing_batch_ids:
                with open(BATCH_CSV, 'a') as f:
                    csv.writer(f).writerow([
                        batch_idx+1, batch_id, len(to_upload),
                        f"https://mineru.net/extract/batch/{batch_id}",
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    ])
                existing_batch_ids.add(batch_id)  # æ·»åŠ åˆ°é›†åˆä¸­
                print(f"  âœ“ æ‰¹æ¬¡è®°å½•å·²ä¿å­˜åˆ°CSV")
            else:
                print(f"  â­ï¸  æ‰¹æ¬¡è®°å½•å·²å­˜åœ¨ï¼Œè·³è¿‡ä¿å­˜")
        
        except Exception as e:
            print(f"  âŒ å¼‚å¸¸: {e}")
        
        time.sleep(1)
    
    print(f"\n{'='*70}")
    print(f"âœ… ä¸Šä¼ å®Œæˆ!")
    print(f"   æ–°ä¸Šä¼ : {uploaded_total} ä¸ªPDF")
    print(f"   å·²è·³è¿‡: {skipped_total} ä¸ªPDFï¼ˆå·²ä¸Šä¼ è¿‡ï¼‰")
    print(f"   çŠ¶æ€è¿½è¸ª: {STATUS_JSON}")
    print(f"   å·²ä¸Šä¼ PDFå·²ç§»è‡³: {PROCESSED_PDF_DIR}")
    print(f"{'='*70}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: python {Path(__file__).name} status")

# ==================== æŸ¥è¯¢å‘½ä»¤ ====================
def cmd_status(args):
    if not BATCH_CSV.exists():
        print("âš ï¸  æœªæ‰¾åˆ°æ‰¹æ¬¡è®°å½•")
        return
    
    with open(BATCH_CSV) as f:
        batches = list(csv.DictReader(f))
    
    print(f"\n{'='*70}\nğŸ“Š æ‰¹æ¬¡å¤„ç†çŠ¶æ€\n{'='*70}\n")
    
    session = create_session()
    for b in batches:
        bid, idx = b['batch_id'], b['batch_index']
        print(f"ğŸ“¦ æ‰¹æ¬¡ {idx}: {bid}")
        
        try:
            r = session.get(f"{MINERU_API_BASE}/extract-results/batch/{bid}",
                          headers=HEADERS, timeout=30)
            if r.status_code == 200 and r.json().get("code") == 0:
                d = r.json()["data"]
                extract_results = d.get("extract_result", [])
                total = len(extract_results)
                done = sum(1 for item in extract_results if item.get("state") == "done")
                processing = sum(1 for item in extract_results if item.get("state") in ["processing", "waiting"])
                failed = sum(1 for item in extract_results if item.get("state") == "failed")
                
                print(f"   æ€»è®¡: {total} ä¸ªæ–‡ä»¶")
                print(f"   âœ… å®Œæˆ: {done}")
                print(f"   â³ å¤„ç†ä¸­: {processing}")
                if failed > 0:
                    print(f"   âŒ å¤±è´¥: {failed}")
                if done == total and total > 0:
                    is_dl = status_mgr.is_downloaded(bid)
                    print(f"   ä¸‹è½½: {'âœ… å·²ä¸‹è½½' if is_dl else 'â¬‡ï¸  å¾…ä¸‹è½½'}")
        except Exception as e:
            print(f"   âŒ {e}")
        print()
    
    stats = status_mgr.get_stats()
    print(f"{'='*70}")
    print(f"ğŸ“Š æ€»è®¡:")
    print(f"   å·²ä¸Šä¼ : {stats['uploaded']} ä¸ªPDF")
    print(f"   å·²ä¸‹è½½: {stats['downloaded']} ä¸ªæ‰¹æ¬¡")
    print(f"   å·²åˆ†æ: {stats['analyzed']} ç¯‡è®ºæ–‡")
    print(f"{'='*70}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: python {Path(__file__).name} download")

# ==================== ä¸‹è½½å‘½ä»¤ ====================
def cmd_download(args):
    out = Path(args.output) if args.output else OUTPUT_DIR
    
    if not BATCH_CSV.exists():
        print("âš ï¸  æœªæ‰¾åˆ°æ‰¹æ¬¡è®°å½•")
        return
    
    with open(BATCH_CSV) as f:
        batches = list(csv.DictReader(f))
    
    print(f"\n{'='*70}\nâ¬‡ï¸  ä¸‹è½½è§£æç»“æœ\n{'='*70}\n")
    
    session = create_session()
    new, skipped = 0, 0
    
    for b in batches:
        bid, idx = b['batch_id'], b['batch_index']
        
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if status_mgr.is_downloaded(bid):
            print(f"ğŸ“¦ æ‰¹æ¬¡ {idx}: {bid}")
            print(f"   â­ï¸  å·²ä¸‹è½½ï¼Œè·³è¿‡\n")
            skipped += 1
            continue
        
        print(f"ğŸ“¦ æ‰¹æ¬¡ {idx}: {bid}")
        
        try:
            r = session.get(f"{MINERU_API_BASE}/extract-results/batch/{bid}",
                          headers=HEADERS, timeout=30)
            
            if r.status_code != 200 or r.json().get("code") != 0:
                print("  âŒ æŸ¥è¯¢å¤±è´¥\n")
                continue
            
            d = r.json()["data"]
            extract_results = d.get("extract_result", [])
            total = len(extract_results)
            done = sum(1 for item in extract_results if item.get("state") == "done")
            
            if done < total:
                print(f"  â³ å¤„ç†ä¸­: {done}/{total} ä¸ªæ–‡ä»¶å®Œæˆ\n")
                continue
            
            # ä¸‹è½½æ–‡ä»¶
            batch_dir = out / f"batch_{idx}"
            batch_dir.mkdir(exist_ok=True)
            success = 0
            
            for f_info in extract_results:
                if f_info.get("state") != "done":
                    continue
                
                did = f_info.get("data_id", "unknown")
                url = f_info.get("full_zip_url")
                if not url:
                    continue
                
                safe = sanitize(did)
                zip_path = batch_dir / f"{safe}.zip"
                extract_dir = batch_dir / safe
                
                if download_file(url, zip_path):
                    if unzip_file(zip_path, extract_dir):
                        zip_path.unlink()
                        success += 1
                        print(f"  âœ… {did}")
            
            if success > 0:
                # æ ‡è®°å·²ä¸‹è½½
                status_mgr.mark_downloaded(bid)
                new += 1
                print(f"  å®Œæˆ: {success} ä¸ªæ–‡ä»¶")
        
        except Exception as e:
            print(f"  âŒ {e}")
        print()
    
    print(f"{'='*70}")
    print(f"âœ… ä¸‹è½½å®Œæˆ!")
    print(f"   æ–°ä¸‹è½½: {new} ä¸ªæ‰¹æ¬¡")
    print(f"   å·²è·³è¿‡: {skipped} ä¸ªæ‰¹æ¬¡ï¼ˆå·²ä¸‹è½½è¿‡ï¼‰")
    print(f"   è¾“å‡ºç›®å½•: {out}")
    print(f"{'='*70}")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: python test_new_pipeline.py")

# ==================== ä¸»å‡½æ•° ====================
def main():
    parser = argparse.ArgumentParser(description="PDF Pipelineï¼ˆæ™ºèƒ½å»é‡ç‰ˆï¼‰")
    sub = parser.add_subparsers(dest='cmd')
    
    up = sub.add_parser('upload', help='ä¸Šä¼ PDFï¼ˆè‡ªåŠ¨å»é‡ï¼‰')
    up.add_argument('-i', '--input', help=f'PDFç›®å½•(é»˜è®¤{PDF_DIR})')
    
    sub.add_parser('status', help='æŸ¥è¯¢å¤„ç†çŠ¶æ€')
    
    dl = sub.add_parser('download', help='ä¸‹è½½ç»“æœï¼ˆè‡ªåŠ¨å»é‡ï¼‰')
    dl.add_argument('-o', '--output', help=f'è¾“å‡ºç›®å½•(é»˜è®¤{OUTPUT_DIR})')
    
    sub.add_parser('stats', help='æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯')
    
    args = parser.parse_args()
    
    if args.cmd:
        print(f"\n{'='*70}")
        print(f"âš™ï¸  é…ç½®ä¿¡æ¯")
        print(f"{'='*70}")
        print(f"MinerU API: {MINERU_API_BASE}")
        print(f"Token: {'âœ“ å·²é…ç½®' if MINERU_TOKEN else 'âœ— æœªé…ç½®'}")
        print(f"PDFç›®å½•: {PDF_DIR}")
        print(f"å·²å¤„ç†ç›®å½•: {PROCESSED_PDF_DIR}")
        print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        print(f"çŠ¶æ€è¿½è¸ª: {STATUS_JSON}")
        print(f"{'='*70}")
    
    if args.cmd == 'upload':
        cmd_upload(args)
    elif args.cmd == 'status':
        cmd_status(args)
    elif args.cmd == 'download':
        cmd_download(args)
    elif args.cmd == 'stats':
        s = status_mgr.get_stats()
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   å·²ä¸Šä¼ : {s['uploaded']} ä¸ªPDF")
        print(f"   å·²ä¸‹è½½: {s['downloaded']} ä¸ªæ‰¹æ¬¡")
        print(f"   å·²åˆ†æ: {s['analyzed']} ç¯‡è®ºæ–‡")
        print(f"\nğŸ“ æ–‡ä»¶ä½ç½®:")
        print(f"   å¾…ä¸Šä¼ : {PDF_DIR}")
        print(f"   å·²ä¸Šä¼ : {PROCESSED_PDF_DIR}")
        print(f"   è§£æç»“æœ: {OUTPUT_DIR}")
        print(f"   çŠ¶æ€æ–‡ä»¶: {STATUS_JSON}")
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
